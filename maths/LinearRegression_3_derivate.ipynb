{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import matplotlib.colors as mcolors\n",
    "from ipywidgets import interact, widgets, interactive, fixed, interact_manual\n",
    "import random\n",
    "import matplotlib.colors as mcolors\n",
    "import time\n",
    "import colorama\n",
    "import pickle\n",
    "\n",
    "with open(\"Teach_linear_regression.pkl\", \"rb\") as f:\n",
    "    (\n",
    "        x, y, COEF_A, COEF_B,\n",
    "        COEF_A_MIN, COEF_A_MAX,\n",
    "        COEF_B_MIN, COEF_B_MAX,\n",
    "        WORST_A_PARAM, WORST_B_PARAM,\n",
    "        MIN_COST, MAX_COST,\n",
    "        MIN_VALUE, MAX_VALUE, NB_ELEMENTS,\n",
    "    ) = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_function_with_derivative(f, x_point=1, h=0.5):\n",
    "    # Generate points for plotting\n",
    "    MINIMUM_Y = -2\n",
    "    x = np.linspace(-1, 3, 1000)\n",
    "    y = f(x)\n",
    "\n",
    "    # Create the plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Plot main function\n",
    "    plt.plot(x, y, 'b-', label='f(x)')\n",
    "\n",
    "    # Calculate points for derivative visualization\n",
    "    x_plus_h = x_point + h\n",
    "    x_minus_h = x_point - h\n",
    "    y_point = f(x_point)\n",
    "    y_plus_h = f(x_plus_h)\n",
    "    y_minus_h = f(x_minus_h)\n",
    "\n",
    "    # Plot points\n",
    "    plt.plot(x_point, y_point, 'X', color='red', label=f'f({x_point:.2f})', markersize=10)\n",
    "    plt.plot(x_point + h, y_plus_h, '+', color='gray', alpha=0.5, markersize=8)\n",
    "    plt.plot(x_point - h, y_minus_h, '+', color='gray', alpha=0.5, markersize=8)\n",
    "\n",
    "    # Plot secant line through the two points\n",
    "    x_line = np.array([x_point - h, x_point + h])\n",
    "    y_line = np.array([y_minus_h, y_plus_h])\n",
    "    plt.plot(x_line, y_line, \"-\", color=\"gray\", linewidth=2)\n",
    "\n",
    "    # bottom_horizontal_line_x = np.array([x_minus_h, x_plus_h])\n",
    "    # bottom_horizontal_line_y = np.array([y_minus_h, y_minus_h])\n",
    "\n",
    "    top_horizontal_line_x = np.array([x_minus_h, x_plus_h])\n",
    "    top_horizontal_line_y = np.array([y_plus_h, y_plus_h])\n",
    "\n",
    "    left_vertical_line_x = np.array([x_minus_h, x_minus_h])\n",
    "    left_vertical_line_y = np.array([y_minus_h, y_plus_h])\n",
    "\n",
    "    h_minus_gray_line_x = np.array([x_minus_h, x_minus_h])\n",
    "    h_minus_gray_line_y = np.array([-10, y_minus_h])\n",
    "\n",
    "    h_plus_gray_line_x = np.array([x_plus_h, x_plus_h])\n",
    "    h_plus_gray_line_y = np.array([-10, y_plus_h])\n",
    "\n",
    "    # right_vertical_line_x = np.array([x_plus_h, x_plus_h])\n",
    "    # right_vertical_line_y = np.array([y_minus_h, y_plus_h])\n",
    "    # Add text annotations for dy and dx with their values\n",
    "    dy = y_plus_h - y_minus_h\n",
    "    dx = x_plus_h - x_minus_h\n",
    "    derivative = dy/dx\n",
    "\n",
    "    # Using matplotlib's text with math mode and custom colors\n",
    "    plt.text(-0.85, 3.7, r\"$\\mathbf{dy} = \" + f\"{dy:.2f}$\", color='orange', fontsize=12)\n",
    "    plt.text(-0.85, 3.3, r\"$\\mathbf{dx} = \" + r\"= 2 \\times h \" + f\" = {dx:.2f} \" + \"$\", color='purple', fontsize=12)\n",
    "\n",
    "    # Add the ratio calculation with colored values and result\n",
    "    plt.text(-0.85, 2.6, \n",
    "             r\"$\\mathbf{\" +\n",
    "             r\"\\frac{dy}{dx} = \" + \n",
    "             r\"\\frac{\" + f\"{dy:.2f}\" + \"}{\" + f\"{dx:.2f}\" + \"}} = \" + \n",
    "             f\"{derivative:.2f}$\",\n",
    "             color='gray', fontsize=14)\n",
    "    # if y_minus_h > y_plus_h:\n",
    "    # plt.plot(bottom_horizontal_line_x, bottom_horizontal_line_y, \":\", color=\"purple\", linewidth=2, label=\"Horizontal line\")\n",
    "    # plt.plot(right_vertical_line_x, right_vertical_line_y, \":\", color=\"orange\", linewidth=2, label=\"Vertical line\")\n",
    "    # else:\n",
    "    plt.plot(top_horizontal_line_x, top_horizontal_line_y, \"-\", color=\"purple\", linewidth=2, label=\"$dx = 2 h$\")\n",
    "    plt.plot(left_vertical_line_x, left_vertical_line_y, \"-\", color=\"orange\", linewidth=2, label=r\"$dy = \\frac{f(x+h) - f(x-h)}{2h}$\")\n",
    "    plt.plot(h_minus_gray_line_x, h_minus_gray_line_y, \":\", color=\"gray\", linewidth=2, label=\"x-h\")\n",
    "    plt.plot(h_plus_gray_line_x, h_plus_gray_line_y, \":\", color=\"gray\", linewidth=2, label=\"x+h\")\n",
    "\n",
    "    plt.text(x_plus_h + 0.05, MINIMUM_Y +0.5, r\"x+h\", color=\"gray\", fontsize=12)\n",
    "    plt.text(x_minus_h - 0.15, MINIMUM_Y + 0.5, r\"x-h\", color=\"gray\", fontsize=12)\n",
    "    # Extend the line in both directions\n",
    "    slope = (y_plus_h - y_minus_h) / (2*h)\n",
    "    x_extended = np.array([x_point - 5, x_point + 5])\n",
    "    y_extended = slope * (x_extended - x_point) + y_point\n",
    "    # Plot both the segment between points and extended line\n",
    "    # plt.plot(x_extended, y_extended, '--', color='gray', label='Extended secant')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.title(f'Function with Derivative Approximation (h={h:.2f})')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('f(x)')\n",
    "    plt.xlim(-1, 3)\n",
    "    plt.ylim(-2, 4)\n",
    "\n",
    "def chaotic_function(x):\n",
    "    return np.sin(5*x**2) * np.cos(x/2) + np.exp(-((x-3)**2)/10) * np.sin(5*x) + 0.5 * np.tanh(x/2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Teach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal\n",
    "\n",
    "Know how we should update theta to reduce loss ?\n",
    "\n",
    "\n",
    "This is : $ \\frac{d \\text{loss}}{d a} $ & $\\frac{d \\text{loss}}{d b} $\n",
    "\n",
    "\n",
    "But, what's the derivate ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dfaa99983f44ee490cf29b62faf63b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=1.0, description='X:', max=2.0, step=0.01), FloatSlider(value=0.15, deâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact(\n",
    "    x_point=widgets.FloatSlider(\n",
    "        value=1,\n",
    "        min=0.0,\n",
    "        max=2.0,\n",
    "        step=0.01,\n",
    "        description=\"X:\",\n",
    "    ),\n",
    "    h=widgets.FloatSlider(\n",
    "        value=0.15,\n",
    "        min=0.01,\n",
    "        max=0.5,\n",
    "        step=0.01,\n",
    "        description=\"h:\",\n",
    "    ),\n",
    ")\n",
    "def interactive_derivative(x_point, h):\n",
    "    # f = lambda x: x**2\n",
    "    f = chaotic_function\n",
    "    plot_function_with_derivative(f, x_point, h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivate intuition\n",
    "  - We can use limits to compute an analytical derivate\n",
    "\n",
    "### $\\frac{df(x)}{dx} = \\lim_{h \\to 0} \\frac{f(x+h) - f(x-h)}{2h}$\n",
    "\n",
    " - Cheat sheet\n",
    "   - Direct value\n",
    "     - Constant: \n",
    "       - $\\frac{d}{dx}[c] = 0$\n",
    "     - Add: \n",
    "       - $\\frac{d}{dx}[f(x) + g(x)] = \\frac{d}{dx}f(x) + \\frac{d}{dx}g(x)$\n",
    "     - Multiply: \n",
    "       - $\\frac{d}{dx}[c \\cdot f(x)] = c \\cdot \\frac{d}{dx}f(x)$\n",
    "     - Power: \n",
    "       - $\\frac{d}{dx}[x^n] = n \\cdot x^{n-1}$\n",
    "   - Chain rule: \n",
    "     - $\\frac{d}{dx}[f(g(x))] = f'(g(x)) \\cdot g'(x) = \\frac{df(x)}{dg(x)} \\frac{dg(x)}{dx}$\n",
    "\n",
    "![Derivatives](./data/derivatives_cheat_sheet.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's apply it to our Linear Regression\n",
    "\n",
    "\n",
    "2 - Let's apply it to our graph\n",
    "  - High level view of our graph of computation\n",
    "![](./data/derivative_draw_0_high_level.jpg)\n",
    "    - Goal: \n",
    "      - d loss / d theta\n",
    "      - Chain rule:\n",
    "        -   d loss / d LinReg\n",
    "        -   d LinReg / d theta\n",
    "  - Let's go further\n",
    "    - Show :\n",
    "      - Detailed computations\n",
    "![](./data/derivative_draw_1_low_level.jpg)\n",
    "      - Full chain rule\n",
    "![](./data/derivative_draw_1.5_chain_rule.jpg)\n",
    "    - The path we will take :\n",
    "![](./data/derivative_draw_2_high_level_path.jpg)\n",
    "      - For each step: d out / d in \n",
    "      - We will need to multiply everything after\n",
    "    - Let's go : \n",
    "      - Obtain the derivatives 1 by 1\n",
    "![](./data/derivative_draw_4_what_to_compute.jpg)\n",
    "      - Multiply everything\n",
    "![](./data/derivative_draw_5_complete.jpg)\n",
    "      - Done !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDEAL: COEF_A = 2.7684667311238043\n",
      "IDEAL: COEF_B = 3.073489251946131\n",
      "\n",
      "Optimizing parameter with \u001b[32mfinite_difference\u001b[39m\n",
      "Time taken: \u001b[33m2.5448598861694336 seconds\u001b[39m \n",
      "\twith finite_difference\n",
      "\ta = 2.245177823764299\n",
      "\tb = 3.588318773979193\n",
      "\t\u001b[31mcost = 0.45820155384552125\u001b[39m\n",
      "\n",
      "\n",
      "Optimizing parameter with \u001b[32mderivative\u001b[39m\n",
      "Time taken: \u001b[33m1.6806817054748535 seconds\u001b[39m \n",
      "\twith derivative\n",
      "\ta = 2.578090939614186\n",
      "\tb = 3.177332679174755\n",
      "\t\u001b[31mcost = 0.371116406168403\u001b[39m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------ Base functions ------------------------------ #\n",
    "\n",
    "def predict(x, thetas):\n",
    "    a, b = thetas\n",
    "    y_hat = (a * x) + b\n",
    "    return y_hat\n",
    "\n",
    "\n",
    "def get_cost(y, y_hat):\n",
    "    return ((y_hat - y) ** 2).mean()\n",
    "\n",
    "\n",
    "# -------------------------------- Derivation -------------------------------- #\n",
    "\n",
    "\n",
    "def get_d_cost_by_d_a(y, y_hat, a):\n",
    "    return 2 * (y_hat - y) * x\n",
    "\n",
    "def get_d_cost_by_d_b(y, y_hat, b):\n",
    "    return 2 * (y_hat - y)\n",
    "\n",
    "def get_parameter_derivative(x, y, a, b):\n",
    "    y_hat = predict(x, (a, b))\n",
    "\n",
    "    d_cost_by_d_a = get_d_cost_by_d_a(y, y_hat, a).mean()\n",
    "    d_cost_by_d_b = get_d_cost_by_d_b(y, y_hat, b).mean()\n",
    "\n",
    "    return d_cost_by_d_a, d_cost_by_d_b\n",
    "\n",
    "\n",
    "# ----------------------------- Finite difference ---------------------------- #\n",
    "\n",
    "\n",
    "def get_parameter_finite_difference(x, y, a, b):\n",
    "    epsilon = 0.5\n",
    "    base_cost = get_cost(y, predict(x, (a, b)))\n",
    "    a_minus_epsilon = a - epsilon\n",
    "    b_minus_epsilon = b - epsilon\n",
    "\n",
    "    cost_a_minus_epsilon = get_cost(y, predict(x, (a_minus_epsilon, b)))\n",
    "    cost_b_minus_epsilon = get_cost(y, predict(x, (a, b_minus_epsilon)))\n",
    "    # print(f\"{base_cost = } {cost_a_epsilon = } {cost_b_epsilon = }\")\n",
    "\n",
    "    cost_a_finite_difference = (base_cost - cost_a_minus_epsilon) / epsilon\n",
    "    cost_b_finite_difference = (base_cost - cost_b_minus_epsilon) / epsilon\n",
    "    # print(f\"{cost_a_finite_difference = } {cost_b_finite_difference = }\")\n",
    "\n",
    "    return cost_a_finite_difference, cost_b_finite_difference\n",
    "\n",
    "\n",
    "def get_parameter_finite_difference_precision(x, y, a, b):\n",
    "    epsilon = 0.5\n",
    "    # base_cost = get_cost(y, predict(x, (a, b)))\n",
    "    a_plus_epsilon = a + epsilon\n",
    "    b_plus_epsilon = b + epsilon\n",
    "\n",
    "    a_minus_epsilon = a - epsilon\n",
    "    b_minus_epsilon = b - epsilon\n",
    "\n",
    "    cost_a_plus_epsilon = get_cost(y, predict(x, (a_plus_epsilon, b)))\n",
    "    cost_b_plus_epsilon = get_cost(y, predict(x, (a, b_plus_epsilon)))\n",
    "    cost_a_minus_epsilon = get_cost(y, predict(x, (a_minus_epsilon, b)))\n",
    "    cost_b_minus_epsilon = get_cost(y, predict(x, (a, b_minus_epsilon)))\n",
    "    # print(f\"{base_cost = } {cost_a_plus_epsilon = } {cost_b_plus_epsilon = } {cost_a_minus_epsilon = } {cost_b_minus_epsilon = }\")\n",
    "\n",
    "    cost_a_finite_difference = (cost_a_plus_epsilon - cost_a_minus_epsilon) / (2 * epsilon)\n",
    "    cost_b_finite_difference = (cost_b_plus_epsilon - cost_b_minus_epsilon) / (2 * epsilon)\n",
    "    # print(f\"{cost_a_finite_difference = } {cost_b_finite_difference = }\")\n",
    "\n",
    "    return cost_a_finite_difference, cost_b_finite_difference\n",
    "\n",
    "\n",
    "# --------------------------------- Optimize --------------------------------- #\n",
    "\n",
    "\n",
    "def optimize_parameter(x, y, a, b, learning_rate, nb_iterations, derivative_method):\n",
    "    print(\n",
    "        f\"\\nOptimizing parameter with {colorama.Fore.GREEN}{derivative_method}{colorama.Fore.RESET}\"\n",
    "    )\n",
    "    start_time = time.time()\n",
    "    for _ in range(nb_iterations):\n",
    "        if derivative_method == \"derivative\":\n",
    "            d_cost_by_d_a, d_cost_by_d_b = get_parameter_derivative(x, y, a, b)\n",
    "        elif derivative_method == \"finite_difference\":\n",
    "            d_cost_by_d_a, d_cost_by_d_b = get_parameter_finite_difference(\n",
    "                x, y, a, b\n",
    "            )\n",
    "        elif derivative_method == \"finite_difference_precision\":\n",
    "            d_cost_by_d_a, d_cost_by_d_b = get_parameter_finite_difference_precision(\n",
    "                x, y, a, b\n",
    "            )\n",
    "        a = a - (learning_rate * d_cost_by_d_a)\n",
    "        b = b - (learning_rate * d_cost_by_d_b)\n",
    "    end_time = time.time()\n",
    "    cost = get_cost(y, predict(x, (a, b)))\n",
    "    print(\n",
    "        f\"Time taken: {colorama.Fore.YELLOW}{end_time - start_time} seconds{colorama.Fore.RESET} \\n\"\n",
    "        f\"\\twith {derivative_method}\\n\"\n",
    "        f\"\\t{a = }\\n\"\n",
    "        f\"\\t{b = }\\n\"\n",
    "        f\"\\t{colorama.Fore.RED}{cost = }{colorama.Fore.RESET}\\n\"\n",
    "    )\n",
    "    return a, b\n",
    "\n",
    "\n",
    "print(f\"IDEAL: {COEF_A = }\")\n",
    "print(f\"IDEAL: {COEF_B = }\")\n",
    "learning_rate = 1e-4\n",
    "nb_iterations = 100_000\n",
    "\n",
    "\n",
    "a, b = optimize_parameter(\n",
    "    x, y, WORST_A_PARAM, WORST_B_PARAM, learning_rate, nb_iterations, \"finite_difference\"\n",
    ")\n",
    "# a, b = optimize_parameter(\n",
    "#     x, y, WORST_A_PARAM, WORST_B_PARAM, learning_rate, nb_iterations, \"finite_difference_precision\"\n",
    "# )\n",
    "a, b = optimize_parameter(\n",
    "    x, y, WORST_A_PARAM, WORST_B_PARAM, learning_rate, nb_iterations, \"derivative\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDEAL: COEF_A = 2.544502850465683\n",
      "IDEAL: COEF_B = 2.39629117885467\n",
      "Time taken: 0.021463394165039062 seconds with derivative = True\n",
      "DERIVATIVE: a = 1.8909630721574668e+176\n",
      "DERIVATIVE: b = 3.781926144314933e+176\n",
      "Time taken: 0.021030664443969727 seconds with derivative = False\n",
      "FINITE DIFFERENCE: a = nan\n",
      "FINITE DIFFERENCE: b = nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1247998/1646815893.py:10: RuntimeWarning: overflow encountered in square\n",
      "  return ((y - y_hat) ** 2).sum()\n",
      "/tmp/ipykernel_1247998/1646815893.py:39: RuntimeWarning: invalid value encountered in scalar subtract\n",
      "  cost_a_finite_difference = (cost_a_epsilon - base_cost)\n",
      "/tmp/ipykernel_1247998/1646815893.py:40: RuntimeWarning: invalid value encountered in scalar subtract\n",
      "  cost_b_finite_difference = (cost_b_epsilon - base_cost)\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

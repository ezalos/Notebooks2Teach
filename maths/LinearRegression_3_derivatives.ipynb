{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import matplotlib.colors as mcolors\n",
    "from ipywidgets import interact, widgets, interactive, fixed, interact_manual\n",
    "import random\n",
    "import matplotlib.colors as mcolors\n",
    "import time\n",
    "import colorama\n",
    "import pickle\n",
    "\n",
    "with open(\"Teach_linear_regression.pkl\", \"rb\") as f:\n",
    "    (\n",
    "        x, y, COEF_A, COEF_B,\n",
    "        COEF_A_MIN, COEF_A_MAX,\n",
    "        COEF_B_MIN, COEF_B_MAX,\n",
    "        WORST_A_PARAM, WORST_B_PARAM,\n",
    "        MIN_COST, MAX_COST,\n",
    "        MIN_VALUE, MAX_VALUE, NB_ELEMENTS,\n",
    "    ) = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_function_with_derivative(f, x_point=1, h=0.5):\n",
    "    # Generate points for plotting\n",
    "    MINIMUM_Y = -2\n",
    "    x = np.linspace(-1, 3, 1000)\n",
    "    y = f(x)\n",
    "\n",
    "    # Create the plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Plot main function\n",
    "    plt.plot(x, y, \"b-\", label=\"f(x)\")\n",
    "\n",
    "    # Calculate points for derivative visualization\n",
    "    x_plus_h = x_point + h\n",
    "    x_minus_h = x_point - h\n",
    "    y_point = f(x_point)\n",
    "    y_plus_h = f(x_plus_h)\n",
    "    y_minus_h = f(x_minus_h)\n",
    "\n",
    "    # Plot points\n",
    "    plt.plot(\n",
    "        x_point, y_point, \"X\", color=\"red\", label=f\"f({x_point:.2f})\", markersize=10\n",
    "    )\n",
    "    plt.plot(x_point + h, y_plus_h, \"+\", color=\"gray\", alpha=0.5, markersize=8)\n",
    "    plt.plot(x_point - h, y_minus_h, \"+\", color=\"gray\", alpha=0.5, markersize=8)\n",
    "\n",
    "    # Plot secant line through the two points\n",
    "    x_line = np.array([x_point - h, x_point + h])\n",
    "    y_line = np.array([y_minus_h, y_plus_h])\n",
    "    plt.plot(x_line, y_line, \"-\", color=\"gray\", linewidth=2)\n",
    "\n",
    "    # bottom_horizontal_line_x = np.array([x_minus_h, x_plus_h])\n",
    "    # bottom_horizontal_line_y = np.array([y_minus_h, y_minus_h])\n",
    "\n",
    "    top_horizontal_line_x = np.array([x_minus_h, x_plus_h])\n",
    "    top_horizontal_line_y = np.array([y_plus_h, y_plus_h])\n",
    "\n",
    "    left_vertical_line_x = np.array([x_minus_h, x_minus_h])\n",
    "    left_vertical_line_y = np.array([y_minus_h, y_plus_h])\n",
    "\n",
    "    h_minus_gray_line_x = np.array([x_minus_h, x_minus_h])\n",
    "    h_minus_gray_line_y = np.array([-10, y_minus_h])\n",
    "\n",
    "    h_plus_gray_line_x = np.array([x_plus_h, x_plus_h])\n",
    "    h_plus_gray_line_y = np.array([-10, y_plus_h])\n",
    "\n",
    "    # right_vertical_line_x = np.array([x_plus_h, x_plus_h])\n",
    "    # right_vertical_line_y = np.array([y_minus_h, y_plus_h])\n",
    "    # Add text annotations for dy and dx with their values\n",
    "    dy = y_plus_h - y_minus_h\n",
    "    dx = x_plus_h - x_minus_h\n",
    "    derivative = dy / dx\n",
    "\n",
    "    # Using matplotlib's text with math mode and custom colors\n",
    "    plt.text(-0.85, 3.7, r\"$\\mathbf{dy} = \" + f\"{dy:.2f}$\", color=\"orange\", fontsize=12)\n",
    "    plt.text(\n",
    "        -0.85,\n",
    "        3.3,\n",
    "        r\"$\\mathbf{dx} = \" + r\"= 2 \\times h \" + f\" = {dx:.2f} \" + \"$\",\n",
    "        color=\"purple\",\n",
    "        fontsize=12,\n",
    "    )\n",
    "\n",
    "    # Add the ratio calculation with colored values and result\n",
    "    plt.text(\n",
    "        -0.85,\n",
    "        2.6,\n",
    "        r\"$\\mathbf{\"\n",
    "        + r\"\\frac{dy}{dx} = \"\n",
    "        + r\"\\frac{\"\n",
    "        + f\"{dy:.2f}\"\n",
    "        + \"}{\"\n",
    "        + f\"{dx:.2f}\"\n",
    "        + \"}} = \"\n",
    "        + f\"{derivative:.2f}$\",\n",
    "        color=\"gray\",\n",
    "        fontsize=14,\n",
    "    )\n",
    "    # if y_minus_h > y_plus_h:\n",
    "    # plt.plot(bottom_horizontal_line_x, bottom_horizontal_line_y, \":\", color=\"purple\", linewidth=2, label=\"Horizontal line\")\n",
    "    # plt.plot(right_vertical_line_x, right_vertical_line_y, \":\", color=\"orange\", linewidth=2, label=\"Vertical line\")\n",
    "    # else:\n",
    "    plt.text(\n",
    "        top_horizontal_line_x[0],\n",
    "        top_horizontal_line_y[0] + 0.1,\n",
    "        r\"$\\mathbf{dx} = 2 h$\",\n",
    "        color=\"purple\",\n",
    "        fontsize=12,\n",
    "    )\n",
    "    plt.plot(\n",
    "        top_horizontal_line_x,\n",
    "        top_horizontal_line_y,\n",
    "        \"-\",\n",
    "        color=\"purple\",\n",
    "        linewidth=2,\n",
    "        label=\"$dx = 2 h$\",\n",
    "    )\n",
    "    plt.plot(\n",
    "        left_vertical_line_x,\n",
    "        left_vertical_line_y,\n",
    "        \"-\",\n",
    "        color=\"orange\",\n",
    "        linewidth=2,\n",
    "        label=r\"$dy = \\frac{f(x+h) - f(x-h)}{2h}$\",\n",
    "    )\n",
    "    plt.plot(\n",
    "        h_minus_gray_line_x,\n",
    "        h_minus_gray_line_y,\n",
    "        \":\",\n",
    "        color=\"gray\",\n",
    "        linewidth=2,\n",
    "        label=\"x-h\",\n",
    "    )\n",
    "    plt.plot(\n",
    "        h_plus_gray_line_x,\n",
    "        h_plus_gray_line_y,\n",
    "        \":\",\n",
    "        color=\"gray\",\n",
    "        linewidth=2,\n",
    "        label=\"x+h\",\n",
    "    )\n",
    "\n",
    "    plt.text(x_plus_h + 0.05, MINIMUM_Y + 0.5, r\"x+h\", color=\"gray\", fontsize=12)\n",
    "    plt.text(x_minus_h - 0.15, MINIMUM_Y + 0.5, r\"x-h\", color=\"gray\", fontsize=12)\n",
    "    # Extend the line in both directions\n",
    "    slope = (y_plus_h - y_minus_h) / (2 * h)\n",
    "    x_extended = np.array([x_point - 5, x_point + 5])\n",
    "    y_extended = slope * (x_extended - x_point) + y_point\n",
    "    # Plot both the segment between points and extended line\n",
    "    # plt.plot(x_extended, y_extended, '--', color='gray', label='Extended secant')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.title(f\"Function with Derivative Approximation (h={h:.2f})\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"f(x)\")\n",
    "    plt.xlim(-1, 3)\n",
    "    plt.ylim(-2, 4)\n",
    "\n",
    "\n",
    "def chaotic_function(x):\n",
    "    return (\n",
    "        np.sin(5 * x**2) * np.cos(x / 2)\n",
    "        + np.exp(-((x - 3) ** 2) / 10) * np.sin(5 * x)\n",
    "        + 0.5 * np.tanh(x / 2)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Teach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal\n",
    "\n",
    "Determine hot to update theta so we reduce loss\n",
    "\n",
    "\n",
    "This is : $ \\frac{d \\text{loss}}{d a} $ & $\\frac{d \\text{loss}}{d b} $\n",
    "\n",
    "\n",
    "But, what's the derivate ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55e6e935dbbe49fa971218454cebd696",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=1.0, description='X:', max=2.0, step=0.01), FloatSlider(value=0.15, deâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "@interact(\n",
    "    x_point=widgets.FloatSlider(\n",
    "        value=1,\n",
    "        min=0.0,\n",
    "        max=2.0,\n",
    "        step=0.01,\n",
    "        description=\"X:\",\n",
    "    ),\n",
    "    h=widgets.FloatSlider(\n",
    "        value=0.15,\n",
    "        min=0.01,\n",
    "        max=0.5,\n",
    "        step=0.01,\n",
    "        description=\"h:\",\n",
    "    ),\n",
    ")\n",
    "def interactive_derivative(x_point, h):\n",
    "    # f = lambda x: x**2\n",
    "    f = chaotic_function\n",
    "    plot_function_with_derivative(f, x_point, h)\n",
    "# TODO: add toggle for dy/dx plot ? Bi axis would be nice\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivate intuition\n",
    "  - We can use limits to compute an analytical derivate\n",
    "\n",
    "### $\\frac{df(x)}{dx} = \\lim_{h \\to 0} \\frac{f(x+h) - f(x-h)}{2h}$\n",
    "\n",
    "\n",
    "Let's demonstrate that $\\frac{d}{dx}[ax] = a$\n",
    "\n",
    "### $\\frac{d}{dx}[ax] = \\lim_{h \\to 0} \\frac{f(x+h) - f(x-h)}{2h}$\n",
    "\n",
    "### $\\frac{d}{dx}[ax] = \\lim_{h \\to 0} \\frac{a(x+h) - a(x-h)}{2h}$\n",
    "\n",
    "### $\\frac{d}{dx}[ax] = \\lim_{h \\to 0} \\frac{ax + ah - (ax - ah)}{2h}$\n",
    "\n",
    "### $\\frac{d}{dx}[ax] = \\lim_{h \\to 0} \\frac{ax + ah - ax + ah}{2h}$\n",
    "\n",
    "### $\\frac{d}{dx}[ax] = \\lim_{h \\to 0} \\frac{2ah}{2h}$\n",
    "\n",
    "### $\\frac{d}{dx}[ax] = \\lim_{h \\to 0} a$\n",
    "\n",
    "### $\\frac{d}{dx}[ax] = a$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## So, which derivatives do we need ?\n",
    "\n",
    "![](./data/derivative_draw_0_high_level.jpg)\n",
    "\n",
    "Add, multiply, power, constant & chain rule\n",
    "\n",
    "### Variables :\n",
    "#### Constant: $\\frac{d}{dx}[c] = 0$\n",
    "#### Multiply: $\\frac{d}{dx}[ax] = a$\n",
    "#### Add: $\\frac{d}{dx}[x+c] = 1$\n",
    "#### Power: $\\frac{d}{dx}[x^n] = n \\cdot x^{n-1}$\n",
    "\n",
    "### Individual Functions :\n",
    "#### Add: $\\frac{d}{dx}[f(x) + g(x)] = \\frac{d}{dx}f(x) + \\frac{d}{dx}g(x)$\n",
    "#### Multiply: $\\frac{d}{dx}[c \\cdot f(x)] = c \\cdot \\frac{d}{dx}f(x)$\n",
    "\n",
    "### Linking functions : \n",
    "### Chain rule: $\\frac{d}{dx}[f(g(x))] = f'(g(x)) \\cdot g'(x) = \\frac{df(x)}{dg(x)} \\cdot \\frac{dg(x)}{dx}$\n",
    "\n",
    "\n",
    "<details>\n",
    "<summary>Click to expand cheat sheet</summary>\n",
    "<!-- ![Derivatives](./data/derivatives_cheat_sheet.png) -->\n",
    "\n",
    "<img src=\"./data/derivatives_cheat_sheet.png\" alt=\"Derivatives cheat sheet\">\n",
    "\n",
    "\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's apply it to our Linear Regression\n",
    "\n",
    "![](./data/derivative_draw_0_high_level.jpg)\n",
    "\n",
    "### GOAL: \n",
    "\n",
    "# $ \\frac{d\\text{loss}}{dA} $\n",
    "\n",
    "# $ \\frac{d\\text{loss}}{dB} $\n",
    "\n",
    "\n",
    "### Let's use the chain rule to break down the problem !\n",
    "\n",
    "![](./data/derivative_draw_1.jpg)\n",
    "\n",
    "What does it mean ?\n",
    "\n",
    "For each step we can deduce :\n",
    "\n",
    "# $\\frac{d \\text{output}}{d \\text{input}} $\n",
    "\n",
    "And we can carry it on :\n",
    "\n",
    "\n",
    "$$\n",
    "\\frac{d\\text{loss}}{da} = \\frac{d\\text{loss}}{d\\Delta} \\cdot \\frac{d\\Delta}{d\\hat{y}} \\cdot \\frac{d\\hat{y}}{du} \\cdot \\frac{du}{dA}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{d\\text{loss}}{db} = \\frac{d\\text{loss}}{d\\Delta} \\cdot \\frac{d\\Delta}{d\\hat{y}} \\cdot \\frac{d\\hat{y}}{db}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "![](./data/derivative_draw_2.jpg)\n",
    "\n",
    "And we can use them to carry on the $d \\text{loss}$\n",
    "\n",
    "\n",
    "Let's see\n",
    "\n",
    "![](./data/derivative_draw_3.jpg)\n",
    "\n",
    "\n",
    "### Let's fill the derivatives : \n",
    "\n",
    "![](./data/derivative_draw_4.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's code it !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------ Base functions ------------------------------ #\n",
    "def predict(x, thetas):\n",
    "    a, b = thetas\n",
    "    y_hat = (a * x) + b\n",
    "    return y_hat\n",
    "\n",
    "\n",
    "def get_cost(y, y_hat):\n",
    "    return ((y_hat - y) ** 2).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------- Derivation -------------------------------- #\n",
    "def get_d_cost_by_d_a(y, y_hat, a):\n",
    "    return 2 * (y_hat - y) * x\n",
    "\n",
    "\n",
    "def get_d_cost_by_d_b(y, y_hat, b):\n",
    "    return 2 * (y_hat - y)\n",
    "\n",
    "\n",
    "def get_parameter_derivative(x, y, a, b):\n",
    "    y_hat = predict(x, (a, b))\n",
    "\n",
    "    d_cost_by_d_a = get_d_cost_by_d_a(y, y_hat, a).mean()\n",
    "    d_cost_by_d_b = get_d_cost_by_d_b(y, y_hat, b).mean()\n",
    "\n",
    "    return d_cost_by_d_a, d_cost_by_d_b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.8476208143819894, 3.3264354715464286)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ----------------------------------- learn ---------------------------------- #\n",
    "def update_parameters(x, y, a, b, learning_rate):\n",
    "    d_cost_by_d_a, d_cost_by_d_b = get_parameter_derivative(x, y, a, b)\n",
    "\n",
    "    a = a - (learning_rate * d_cost_by_d_a)\n",
    "    b = b - (learning_rate * d_cost_by_d_b)\n",
    "    return a, b\n",
    "\n",
    "a, b = 0, 0\n",
    "for i in range(1_000):\n",
    "    a, b = update_parameters(x, y, a, b, learning_rate=0.001)\n",
    "a, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's print some details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial    mean_loss =  21.94 | total loss    2193.53\n",
      "  step   0 mean_loss =  20.85 | total loss    2085.18\n",
      "  step  10 mean_loss =  12.59 | total loss    1259.50\n",
      "  step  20 mean_loss =   7.66 | total loss     765.51\n",
      "  step  30 mean_loss =   4.70 | total loss     469.96\n",
      "  step  40 mean_loss =   2.93 | total loss     293.13\n",
      "  step  50 mean_loss =   1.87 | total loss     187.33\n",
      "  step  60 mean_loss =   1.24 | total loss     124.02\n",
      "  step  70 mean_loss =   0.86 | total loss      86.13\n",
      "  step  80 mean_loss =   0.63 | total loss      63.45\n",
      "  step  90 mean_loss =   0.50 | total loss      49.88\n",
      "Final      mean_loss =   0.42 | total loss      42.38\n"
     ]
    }
   ],
   "source": [
    "def learn_with_prints(x, y, a, b, learning_rate, nb_iterations):\n",
    "    mean_loss = get_cost(y, predict(x, (a, b)))\n",
    "    print(f\"Initial    {mean_loss = :6.2f} | total loss {mean_loss * len(x):10.2f}\")\n",
    "\n",
    "    for i in range(nb_iterations):\n",
    "        a, b = update_parameters(x, y, a, b, learning_rate)\n",
    "\n",
    "        mean_loss = get_cost(y, predict(x, (a, b)))\n",
    "        if i % 10 == 0:\n",
    "            print(f\"  step {i:3d} {mean_loss = :6.2f} | total loss {mean_loss * len(x):10.2f}\")\n",
    "    mean_loss = get_cost(y, predict(x, (a, b)))\n",
    "    print(f\"Final      {mean_loss = :6.2f} | total loss {mean_loss * len(x):10.2f}\")\n",
    "\n",
    "\n",
    "learn_with_prints(x, y, 0, 0, 1e-2, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does it compare to the finite differentiation method ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------- Finite difference ---------------------------- #\n",
    "\n",
    "def get_cost_for_parameter(x, y, a, b):\n",
    "    return get_cost(y, predict(x, (a, b)))\n",
    "\n",
    "\n",
    "def get_parameter_finite_difference(x, y, a, b):\n",
    "    epsilon = 0.5\n",
    "    \n",
    "    cost_base = get_cost_for_parameter(x, y, a, b)\n",
    "    cost_a_minus_epsilon = get_cost_for_parameter(x, y, a - epsilon, b)\n",
    "    cost_b_minus_epsilon = get_cost_for_parameter(x, y, a, b - epsilon)\n",
    "    \n",
    "    cost_a_finite_difference = (cost_base - cost_a_minus_epsilon) / epsilon\n",
    "    cost_b_finite_difference = (cost_base - cost_b_minus_epsilon) / epsilon\n",
    "\n",
    "    return cost_a_finite_difference, cost_b_finite_difference\n",
    "\n",
    "\n",
    "def get_parameter_finite_difference_precision(x, y, a, b):\n",
    "    epsilon = 0.5\n",
    "\n",
    "    cost_a_minus_epsilon = get_cost_for_parameter(x, y, a - epsilon, b)\n",
    "    cost_b_minus_epsilon = get_cost_for_parameter(x, y, a, b - epsilon)\n",
    "    cost_a_plus_epsilon = get_cost_for_parameter(x, y, a + epsilon, b)\n",
    "    cost_b_plus_epsilon = get_cost_for_parameter(x, y, a, b + epsilon)\n",
    "\n",
    "    cost_a_finite_difference = (cost_a_plus_epsilon - cost_a_minus_epsilon) / (2 * epsilon)\n",
    "    cost_b_finite_difference = (cost_b_plus_epsilon - cost_b_minus_epsilon) / (2 * epsilon)\n",
    "\n",
    "    return cost_a_finite_difference, cost_b_finite_difference\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's race them !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDEAL: COEF_A = 2.814443737618271\n",
      "IDEAL: COEF_B = 3.1947961308492467\n",
      "\n",
      "Optimizing parameter with \u001b[32mfinite_difference\u001b[39m\n",
      "Time taken: \u001b[33m2.3501152992248535 seconds\u001b[39m \n",
      "\ta = 1.9148347546029918\n",
      "\tb = 3.8929045616776703\n",
      "\t\u001b[31mcost = 0.3640424435322637\u001b[39m\n",
      "\n",
      "\n",
      "Optimizing parameter with \u001b[32mderivative\u001b[39m\n",
      "Time taken: \u001b[33m1.5655572414398193 seconds\u001b[39m \n",
      "\ta = 2.2477478704529146\n",
      "\tb = 3.4819184668732155\n",
      "\t\u001b[31mcost = 0.2870335946983811\u001b[39m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------- Optimize --------------------------------- #\n",
    "\n",
    "\n",
    "def optimize_parameter(x, y, a, b, learning_rate, nb_iterations, derivative_method):\n",
    "    print(\n",
    "        f\"\\nOptimizing parameter with {colorama.Fore.GREEN}{derivative_method}{colorama.Fore.RESET}\"\n",
    "    )\n",
    "    start_time = time.time()\n",
    "    for _ in range(nb_iterations):\n",
    "        if derivative_method == \"derivative\":\n",
    "            d_cost_by_d_a, d_cost_by_d_b = get_parameter_derivative(x, y, a, b)\n",
    "        elif derivative_method == \"finite_difference\":\n",
    "            d_cost_by_d_a, d_cost_by_d_b = get_parameter_finite_difference(x, y, a, b)\n",
    "        elif derivative_method == \"finite_difference_precision\":\n",
    "            d_cost_by_d_a, d_cost_by_d_b = get_parameter_finite_difference_precision(\n",
    "                x, y, a, b\n",
    "            )\n",
    "        a = a - (learning_rate * d_cost_by_d_a)\n",
    "        b = b - (learning_rate * d_cost_by_d_b)\n",
    "    end_time = time.time()\n",
    "    cost = get_cost(y, predict(x, (a, b)))\n",
    "    print(\n",
    "        f\"Time taken: {colorama.Fore.YELLOW}{end_time - start_time} seconds{colorama.Fore.RESET} \\n\"\n",
    "        # f\"\\twith {derivative_method}\\n\"\n",
    "        f\"\\t{a = }\\n\"\n",
    "        f\"\\t{b = }\\n\"\n",
    "        f\"\\t{colorama.Fore.RED}{cost = }{colorama.Fore.RESET}\\n\"\n",
    "    )\n",
    "    return a, b\n",
    "\n",
    "\n",
    "print(f\"IDEAL: {COEF_A = }\")\n",
    "print(f\"IDEAL: {COEF_B = }\")\n",
    "learning_rate = 1e-4\n",
    "nb_iterations = 100_000\n",
    "\n",
    "\n",
    "a, b = optimize_parameter(\n",
    "    x,\n",
    "    y,\n",
    "    WORST_A_PARAM,\n",
    "    WORST_B_PARAM,\n",
    "    learning_rate,\n",
    "    nb_iterations,\n",
    "    \"finite_difference\",\n",
    ")\n",
    "# a, b = optimize_parameter(\n",
    "#     x, y, WORST_A_PARAM, WORST_B_PARAM, learning_rate, nb_iterations, \"finite_difference_precision\"\n",
    "# )\n",
    "a, b = optimize_parameter(\n",
    "    x, y, WORST_A_PARAM, WORST_B_PARAM, learning_rate, nb_iterations, \"derivative\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

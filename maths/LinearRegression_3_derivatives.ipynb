{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import matplotlib.colors as mcolors\n",
    "from ipywidgets import interact, widgets, interactive, fixed, interact_manual\n",
    "import random\n",
    "import matplotlib.colors as mcolors\n",
    "import time\n",
    "import colorama\n",
    "import pickle\n",
    "\n",
    "with open(\"Teach_linear_regression.pkl\", \"rb\") as f:\n",
    "    (\n",
    "        x, y, COEF_A, COEF_B,\n",
    "        COEF_A_MIN, COEF_A_MAX,\n",
    "        COEF_B_MIN, COEF_B_MAX,\n",
    "        WORST_A_PARAM, WORST_B_PARAM,\n",
    "        MIN_COST, MAX_COST,\n",
    "        MIN_VALUE, MAX_VALUE, NB_ELEMENTS,\n",
    "    ) = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_by_dx(f, x, h):\n",
    "    return (f(x + h) - f(x - h)) / (2 * h)\n",
    "\n",
    "def plot_function_with_derivative(f, x_point=1, h=0.5, plot_triangle=False, plot_df_by_dx=False):\n",
    "    # Generate points for plotting\n",
    "    MINIMUM_Y = -2\n",
    "    x = np.linspace(-1, 3, 1000)\n",
    "    y = f(x)\n",
    "\n",
    "    # Create the plot with twin axes\n",
    "    fig, ax1 = plt.subplots(figsize=(12, 6))\n",
    "    ax2 = ax1.twinx() if plot_df_by_dx else None\n",
    "\n",
    "    if plot_df_by_dx:\n",
    "        # Plot derivative on second axis\n",
    "        df_by_dx = get_df_by_dx(f, x, h)\n",
    "        # Split into positive and negative parts\n",
    "        pos_mask = df_by_dx > 0\n",
    "        neg_mask = ~pos_mask\n",
    "\n",
    "        # print(f\"{x.shape = }\")\n",
    "        # print(f\"{df_by_dx.shape = }\")\n",
    "        # print(f\"{x[pos_mask].shape = }\")\n",
    "        # print(f\"{df_by_dx[pos_mask].shape = }\")\n",
    "        # print(f\"{x[neg_mask].shape = }\")\n",
    "        # print(f\"{df_by_dx[neg_mask].shape = }\")\n",
    "\n",
    "        # Plot positive values in green\n",
    "        ax2.scatter(x[pos_mask], df_by_dx[pos_mask], c='g', marker='.', label=\"f'(x) > 0\", alpha=0.25)\n",
    "        # Plot negative values in red\n",
    "        ax2.scatter(x[neg_mask], df_by_dx[neg_mask], c='r', marker='.', label=\"f'(x) < 0\", alpha=0.25)\n",
    "\n",
    "        ax2.set_ylabel(r\"$\\frac{df(x)}{dx}$\", color=\"g\")\n",
    "        ax2.tick_params(axis='y', labelcolor=\"g\")\n",
    "\n",
    "    # Plot main function on first axis\n",
    "    ax1.plot(x, y, \"black\", label=\"f(x)\", alpha=0.5)\n",
    "    # Calculate points for derivative visualization\n",
    "    x_plus_h = x_point + h\n",
    "    x_minus_h = x_point - h\n",
    "    y_point = f(x_point)\n",
    "    y_plus_h = f(x_plus_h)\n",
    "    y_minus_h = f(x_minus_h)\n",
    "\n",
    "    # Plot points\n",
    "    ax1.plot(\n",
    "        x_point, y_point, \"X\", color=\"orange\", label=f\"f({x_point:.2f})\", markersize=10\n",
    "    )\n",
    "    ax1.plot([x_point, x_point], [MINIMUM_Y, y_point], color=\"orange\", alpha=0.5)\n",
    "\n",
    "    if plot_triangle:\n",
    "        ax1.plot(x_point + h, y_plus_h, \"+\", color=\"gray\", alpha=0.5, markersize=8)\n",
    "        ax1.plot(x_point - h, y_minus_h, \"+\", color=\"gray\", alpha=0.5, markersize=8)\n",
    "\n",
    "        # Plot secant line through the two points\n",
    "        x_line = np.array([x_point - h, x_point + h])\n",
    "        y_line = np.array([y_minus_h, y_plus_h])\n",
    "        ax1.plot(x_line, y_line, \"-\", color=\"purple\", linewidth=2)\n",
    "\n",
    "        top_horizontal_line_x = np.array([x_minus_h, x_plus_h])\n",
    "        top_horizontal_line_y = np.array([y_plus_h, y_plus_h])\n",
    "\n",
    "        left_vertical_line_x = np.array([x_minus_h, x_minus_h])\n",
    "        left_vertical_line_y = np.array([y_minus_h, y_plus_h])\n",
    "\n",
    "        h_minus_gray_line_x = np.array([x_minus_h, x_minus_h])\n",
    "        h_minus_gray_line_y = np.array([-10, y_minus_h])\n",
    "\n",
    "        h_plus_gray_line_x = np.array([x_plus_h, x_plus_h])\n",
    "        h_plus_gray_line_y = np.array([-10, y_plus_h])\n",
    "\n",
    "        # Add text annotations for dy and dx with their values\n",
    "        dy = y_plus_h - y_minus_h\n",
    "        dx = x_plus_h - x_minus_h\n",
    "        derivative = dy / dx\n",
    "\n",
    "        # Using matplotlib's text with math mode and custom colors\n",
    "        ax1.text(-0.85, 3.7, r\"$\\mathbf{dy} = \" + f\"{dy:.2f}$\", color=\"red\", fontsize=12)\n",
    "        ax1.text(\n",
    "            -0.85,\n",
    "            3.3,\n",
    "            r\"$\\mathbf{dx} = \" + r\" 2 \\times h \" + f\" = {dx:.2f} \" + \"$\",\n",
    "            color=\"blue\",\n",
    "            fontsize=12,\n",
    "        )\n",
    "\n",
    "        # Add the ratio calculation with colored values and result\n",
    "        ax1.text(\n",
    "            -0.85,\n",
    "            2.6,\n",
    "            r\"$\\mathbf{\"\n",
    "            + r\"\\frac{dy}{dx} = \"\n",
    "            + r\"\\frac{\"\n",
    "            + f\"{dy:.2f}\"\n",
    "            + \"}{\"\n",
    "            + f\"{dx:.2f}\"\n",
    "            + \"}} = \"\n",
    "            + f\"{derivative:.2f}$\",\n",
    "            color=\"purple\",\n",
    "            fontsize=14,\n",
    "        )\n",
    "\n",
    "        ax1.text(\n",
    "            top_horizontal_line_x[0],\n",
    "            top_horizontal_line_y[0] + 0.1,\n",
    "            r\"$\\mathbf{dx} = 2 h$\",\n",
    "            color=\"blue\",\n",
    "            fontsize=12,\n",
    "        )\n",
    "        ax1.plot(\n",
    "            top_horizontal_line_x,\n",
    "            top_horizontal_line_y,\n",
    "            \"-\",\n",
    "            color=\"blue\",\n",
    "            linewidth=2,\n",
    "            label=\"$dx = 2 h$\",\n",
    "        )\n",
    "        ax1.plot(\n",
    "            left_vertical_line_x,\n",
    "            left_vertical_line_y,\n",
    "            \"-\",\n",
    "            color=\"red\",\n",
    "            linewidth=2,\n",
    "            label=r\"$dy = \\frac{f(x+h) - f(x-h)}{2h}$\",\n",
    "        )\n",
    "        ax1.plot(\n",
    "            h_minus_gray_line_x,\n",
    "            h_minus_gray_line_y,\n",
    "            \":\",\n",
    "            color=\"gray\",\n",
    "            linewidth=2,\n",
    "            label=\"x-h\",\n",
    "        )\n",
    "        ax1.plot(\n",
    "            h_plus_gray_line_x,\n",
    "            h_plus_gray_line_y,\n",
    "            \":\",\n",
    "            color=\"gray\",\n",
    "            linewidth=2,\n",
    "            label=\"x+h\",\n",
    "        )\n",
    "\n",
    "        ax1.text(x_plus_h + 0.05, MINIMUM_Y + 0.5, r\"x+h\", color=\"gray\", fontsize=12)\n",
    "        ax1.text(x_minus_h - 0.15, MINIMUM_Y + 0.5, r\"x-h\", color=\"gray\", fontsize=12)\n",
    "\n",
    "    ax1.grid(True)\n",
    "    plt.title(f\"Function with Derivative Approximation (h={h:.2f})\")\n",
    "    ax1.set_xlabel(\"x\")\n",
    "    ax1.set_ylabel(\"f(x)\", color='b')\n",
    "    ax1.tick_params(axis='y', labelcolor='b')\n",
    "\n",
    "    ax1.set_xlim(-1, 3)\n",
    "    ax1.set_ylim(-2, 4)\n",
    "\n",
    "\n",
    "def chaotic_function(x):\n",
    "    return (\n",
    "        np.sin(5 * x**2) * np.cos(x / 2)\n",
    "        + np.exp(-((x - 3) ** 2) / 10) * np.sin(5 * x)\n",
    "        + 0.5 * np.tanh(x / 2)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Teach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal\n",
    "\n",
    "Determine how to update A & B so we reduce loss\n",
    "\n",
    "\n",
    "This is : $ \\frac{d \\text{loss}}{d a} $ & $\\frac{d \\text{loss}}{d b} $\n",
    "\n",
    "\n",
    "But, what's the derivate ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e221b9326da45138f7483cddef6161f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.75, description='X:', max=2.0, step=0.01), FloatSlider(value=0.25, dâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact(\n",
    "    x_point=widgets.FloatSlider(\n",
    "        value=0.75,\n",
    "        min=0.0,\n",
    "        max=2.0,\n",
    "        step=0.01,\n",
    "        description=\"X:\",\n",
    "    ),\n",
    "    h=widgets.FloatSlider(\n",
    "        value=0.25,\n",
    "        min=0.01,\n",
    "        max=0.5,\n",
    "        step=0.01,\n",
    "        description=\"h:\",\n",
    "    ),\n",
    "    plot_triangle=widgets.Checkbox(\n",
    "        value=True,\n",
    "        description=\"Plot triangle\",\n",
    "    ),\n",
    "    plot_df_by_dx=widgets.Checkbox(\n",
    "        value=False,\n",
    "        description=\"Plot df/dx\",\n",
    "    ),\n",
    ")\n",
    "def interactive_derivative(x_point, h, plot_triangle, plot_df_by_dx):\n",
    "    def f(x):\n",
    "        if isinstance(x, np.ndarray):\n",
    "            y = np.copy(x)\n",
    "            for i in range(len(x)):\n",
    "                if x[i] < 1:\n",
    "                    y[i] = 2*x[i]\n",
    "                else:\n",
    "                    y[i] = x[i]/2\n",
    "            return y\n",
    "        else:\n",
    "            return 2*x if x < 1 else x/2\n",
    "\n",
    "    f = lambda x: x**2\n",
    "    # f = lambda x: 2*x[x<1]+1 + 3*x[x>=1]+1\n",
    "    f = chaotic_function\n",
    "    plot_function_with_derivative(f, x_point, h, plot_triangle, plot_df_by_dx)\n",
    "\n",
    "# AJOUTER L'ANGLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivate intuition\n",
    "  - We can use limits to compute an analytical derivate\n",
    "\n",
    "### $\\frac{df(x)}{dx} = \\lim_{h \\to 0} \\frac{f(x+h) - f(x-h)}{2h}$\n",
    "\n",
    "\n",
    "Let's demonstrate that $\\frac{d}{dx}[ax] = a$\n",
    "\n",
    "### $\\frac{d}{dx}[ax] = \\lim_{h \\to 0} \\frac{f(x+h) - f(x-h)}{2h}$\n",
    "\n",
    "### $\\frac{d}{dx}[ax] = \\lim_{h \\to 0} \\frac{a(x+h) - a(x-h)}{2h}$\n",
    "\n",
    "### $\\frac{d}{dx}[ax] = \\lim_{h \\to 0} \\frac{ax + ah - (ax - ah)}{2h}$\n",
    "\n",
    "### $\\frac{d}{dx}[ax] = \\lim_{h \\to 0} \\frac{ax + ah - ax + ah}{2h}$\n",
    "\n",
    "### $\\frac{d}{dx}[ax] = \\lim_{h \\to 0} \\frac{2ah}{2h}$\n",
    "\n",
    "### $\\frac{d}{dx}[ax] = \\lim_{h \\to 0} a$\n",
    "\n",
    "### $\\frac{d}{dx}[ax] = a$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## So, which derivatives do we need ?\n",
    "\n",
    "![](./data/derivative_draw_0_high_level.jpg)\n",
    "\n",
    "Add, multiply, power, constant & chain rule.\n",
    "\n",
    "## Variables derivatives formulas :\n",
    "\n",
    "#### Constant: $\\frac{d}{dx}[c] = 0$\n",
    "\n",
    "$f(x)=-4$ \n",
    "\n",
    "---> $\\frac{df(x)}{dx}=0$\n",
    "\n",
    "#### Multiply: $\\frac{d}{dx}[ax] = a$\n",
    "\n",
    "$f(x)=2x$ \n",
    "\n",
    "---> $\\frac{df(x)}{dx}=2$\n",
    "\n",
    "#### Add: $\\frac{d}{dx}[x+c] = 1$\n",
    "\n",
    "$f(x)=x+5$ \n",
    "\n",
    "---> $\\frac{df(x)}{dx}=1$\n",
    "\n",
    "#### Power: $\\frac{d}{dx}[x^n] = n \\cdot x^{n-1}$\n",
    "\n",
    "$f(x)=x^3$ \n",
    "\n",
    "---> $\\frac{df(x)}{dx}=3x^2$\n",
    "\n",
    "## Functions : how to solve ? Chain rule !\n",
    "\n",
    "$ \\frac{df(x)}{dx} = 5 \\cdot x^2 + 3 $\n",
    "\n",
    "To derivate, let's break it down in 3 functions : \n",
    "\n",
    "$ f_1(x) = x^2 = y_1 $\n",
    "\n",
    "$ f_2(y_1) = 5 \\cdot y_1 = y_2 $\n",
    "\n",
    "$ f_3(y_2) = y_2 + 3 = y_3$\n",
    "\n",
    "\n",
    "These functions are dependent on each-others :\n",
    "\n",
    "$ f(x) = f_3(f_2(f_1(x))) = y$\n",
    "\n",
    "The result of one function, is used by the next\n",
    "\n",
    "$ x \\rightarrow [f_1(x) = y_1] \\rightarrow [f_2(y_1) = y_2] \\rightarrow [f_3(y_2) = y_3] \\rightarrow y $\n",
    "\n",
    "And so is the derivate : \n",
    "\n",
    "## $ \\frac{df(x)}{dx} = \\frac{df_3(f_2(f_1(x)))}{dx} = \\frac{df_1(x)}{dx} \\cdot \\frac{df_2(y_1)}{dy_1} \\cdot \\frac{df_3(y_2)}{dy_2} $\n",
    "\n",
    "Every time we look at how the output change, when the input change : \n",
    "\n",
    "# $ \\frac{d_{out}}{d_{in}} $\n",
    "\n",
    "And this change is carried over : \n",
    "\n",
    "$ x \\leftarrow \\Delta y_1 \\leftarrow \\Delta y_2 \\leftarrow \\Delta y_3 \\leftarrow \\Delta y $\n",
    "\n",
    "\n",
    "### Chain rule: $\\frac{d}{dx}[f(g(x))] = \\frac{df(x)}{dg(x)} \\cdot \\frac{dg(x)}{dx}$\n",
    "\n",
    "So let's solve our example : \n",
    "\n",
    "## $ f_1(x) = x^2 $\n",
    "\n",
    "Rule: $\\frac{d}{dx}[x^n] = n \\cdot x^{n-1}$\n",
    "\n",
    "Derivative: $\\frac{df_1(x)}{dx} = 2x $\n",
    "\n",
    "\n",
    "## $ f_2(y_1) = 5 \\cdot y_1 $\n",
    "\n",
    "Rule: $\\frac{d}{dx}[ax] = a$\n",
    "\n",
    "Derivative: $\\frac{df_2(y_1)}{dy_1} = 5 $\n",
    "\n",
    "\n",
    "## $ f_3(y_2) = y_2 + 3 $\n",
    "\n",
    "Rule: $\\frac{d}{dx}[x+c] = 1$\n",
    "\n",
    "Derivative: $\\frac{df_3(y_2)}{dy_2} = 1 $\n",
    "\n",
    "\n",
    "## $\\frac{df_3(f_2(f_1(x)))}{dx}$\n",
    "\n",
    "Rule: $\\frac{d}{dx}[f(g(x))] = \\frac{df(x)}{dg(x)} \\cdot \\frac{dg(x)}{dx}$\n",
    "\n",
    "Derivative: \n",
    "\n",
    "$ \\frac{df_3(f_2(f_1(x)))}{dx} = \\frac{df_1(x)}{dx} \\cdot \\frac{df_2(y_1)}{dy_1} \\cdot \\frac{df_3(y_2)}{dy_2} $\n",
    "\n",
    "$ \\frac{df(x)}{dx} = 2x \\cdot 5 \\cdot 1 $\n",
    "\n",
    "$ \\frac{df(x)}{dx} = 10 x $\n",
    "\n",
    "\n",
    "\n",
    "<details>\n",
    "<summary>Click to expand cheat sheet</summary>\n",
    "<!-- ![Derivatives](./data/derivatives_cheat_sheet.png) -->\n",
    "\n",
    "<img src=\"./data/derivatives_cheat_sheet.png\" alt=\"Derivatives cheat sheet\">\n",
    "\n",
    "\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's apply it to our Linear Regression\n",
    "\n",
    "![](./data/derivative_draw_0_high_level.jpg)\n",
    "\n",
    "### GOAL: \n",
    "\n",
    "# $ \\frac{d\\text{loss}}{dA} $ AND $ \\frac{d\\text{loss}}{dB} $\n",
    "\n",
    "\n",
    "### Let's use the chain rule to break down the problem !\n",
    "\n",
    "![](./data/derivative_draw_1.jpg)\n",
    "\n",
    "What does it mean ?\n",
    "\n",
    "For each step we can deduce :\n",
    "\n",
    "# $\\frac{d \\text{output}}{d \\text{input}} $\n",
    "\n",
    "And we can carry it on :\n",
    "\n",
    "\n",
    "$$\n",
    "\\frac{d\\text{loss}}{da} = \\frac{d\\text{loss}}{d\\Delta} \\cdot \\frac{d\\Delta}{d\\hat{y}} \\cdot \\frac{d\\hat{y}}{du} \\cdot \\frac{du}{dA}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{d\\text{loss}}{db} = \\frac{d\\text{loss}}{d\\Delta} \\cdot \\frac{d\\Delta}{d\\hat{y}} \\cdot \\frac{d\\hat{y}}{db}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "![](./data/derivative_draw_2.jpg)\n",
    "\n",
    "And we can use them to carry on the $d \\text{loss}$\n",
    "\n",
    "\n",
    "Let's see\n",
    "\n",
    "![](./data/derivative_draw_3.jpg)\n",
    "\n",
    "Todo: Add black line for simplification\n",
    "\n",
    "\n",
    "### Let's fill the derivatives : \n",
    "\n",
    "Constant: $\\frac{d}{dx}[c] = 0$\n",
    "\n",
    "Multiply: $\\frac{d}{dx}[ax] = a$\n",
    "\n",
    "Add: $\\frac{d}{dx}[x+c] = 1$\n",
    "\n",
    "Power: $\\frac{d}{dx}[x^n] = n \\cdot x^{n-1}$\n",
    "\n",
    "![](./data/derivative_draw_4.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's code it !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------ Base functions ------------------------------ #\n",
    "def predict(x, thetas):\n",
    "    a, b = thetas\n",
    "    y_hat = (a * x) + b\n",
    "    return y_hat\n",
    "\n",
    "\n",
    "def get_cost(y, y_hat):\n",
    "    return ((y_hat - y) ** 2).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------- Derivation -------------------------------- #\n",
    "def get_d_cost_by_d_a(y, y_hat, a):\n",
    "    return 2 * (y_hat - y) * x\n",
    "\n",
    "\n",
    "def get_d_cost_by_d_b(y, y_hat, b):\n",
    "    return 2 * (y_hat - y)\n",
    "\n",
    "\n",
    "def get_parameter_derivative(x, y, a, b):\n",
    "    y_hat = predict(x, (a, b))\n",
    "\n",
    "    d_cost_by_d_a = get_d_cost_by_d_a(y, y_hat, a).mean()\n",
    "    d_cost_by_d_b = get_d_cost_by_d_b(y, y_hat, b).mean()\n",
    "\n",
    "    return d_cost_by_d_a, d_cost_by_d_b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.8367774745854646, 3.0880909646325705)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ----------------------------------- learn ---------------------------------- #\n",
    "def update_parameters(x, y, a, b, learning_rate):\n",
    "    d_cost_by_d_a, d_cost_by_d_b = get_parameter_derivative(x, y, a, b)\n",
    "\n",
    "    a = a - (learning_rate * d_cost_by_d_a)\n",
    "    b = b - (learning_rate * d_cost_by_d_b)\n",
    "    return a, b\n",
    "\n",
    "a, b = 0, 0\n",
    "for i in range(1_000):\n",
    "    a, b = update_parameters(x, y, a, b, learning_rate=0.001)\n",
    "a, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's print some details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial    mean_loss =  19.70 | total loss    1969.87\n",
      "  step   0 mean_loss =  18.73 | total loss    1873.41\n",
      "  step  10 mean_loss =  11.38 | total loss    1138.23\n",
      "  step  20 mean_loss =   6.98 | total loss     698.29\n",
      "  step  30 mean_loss =   4.35 | total loss     434.99\n",
      "  step  40 mean_loss =   2.77 | total loss     277.36\n",
      "  step  50 mean_loss =   1.83 | total loss     182.96\n",
      "  step  60 mean_loss =   1.26 | total loss     126.39\n",
      "  step  70 mean_loss =   0.92 | total loss      92.44\n",
      "  step  80 mean_loss =   0.72 | total loss      72.05\n",
      "  step  90 mean_loss =   0.60 | total loss      59.75\n",
      "  step 100 mean_loss =   0.52 | total loss      52.31\n",
      "  step 110 mean_loss =   0.48 | total loss      47.78\n",
      "  step 120 mean_loss =   0.45 | total loss      44.98\n",
      "  step 130 mean_loss =   0.43 | total loss      43.22\n",
      "  step 140 mean_loss =   0.42 | total loss      42.10\n",
      "  step 150 mean_loss =   0.41 | total loss      41.35\n",
      "  step 160 mean_loss =   0.41 | total loss      40.82\n",
      "  step 170 mean_loss =   0.40 | total loss      40.44\n",
      "  step 180 mean_loss =   0.40 | total loss      40.13\n",
      "  step 190 mean_loss =   0.40 | total loss      39.89\n",
      "Final      mean_loss =   0.40 | total loss      39.69\n"
     ]
    }
   ],
   "source": [
    "def learn_with_prints(x, y, a, b, learning_rate, nb_iterations):\n",
    "    mean_loss = get_cost(y, predict(x, (a, b)))\n",
    "    print(f\"Initial    {mean_loss = :6.2f} | total loss {mean_loss * len(x):10.2f}\")\n",
    "\n",
    "    for i in range(nb_iterations):\n",
    "        a, b = update_parameters(x, y, a, b, learning_rate)\n",
    "\n",
    "        mean_loss = get_cost(y, predict(x, (a, b)))\n",
    "        if i % 10 == 0:\n",
    "            print(f\"  step {i:3d} {mean_loss = :6.2f} | total loss {mean_loss * len(x):10.2f}\")\n",
    "    mean_loss = get_cost(y, predict(x, (a, b)))\n",
    "    print(f\"Final      {mean_loss = :6.2f} | total loss {mean_loss * len(x):10.2f}\")\n",
    "\n",
    "\n",
    "learn_with_prints(x, y, 0, 0, 1e-2, 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does it compare to the finite differentiation method ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------- Finite difference ---------------------------- #\n",
    "\n",
    "def get_cost_for_parameter(x, y, a, b):\n",
    "    return get_cost(y, predict(x, (a, b)))\n",
    "\n",
    "\n",
    "def get_parameter_finite_difference(x, y, a, b):\n",
    "    epsilon = 0.5\n",
    "    \n",
    "    cost_base = get_cost_for_parameter(x, y, a, b)\n",
    "    cost_a_minus_epsilon = get_cost_for_parameter(x, y, a - epsilon, b)\n",
    "    cost_b_minus_epsilon = get_cost_for_parameter(x, y, a, b - epsilon)\n",
    "    \n",
    "    cost_a_finite_difference = (cost_base - cost_a_minus_epsilon) / epsilon\n",
    "    cost_b_finite_difference = (cost_base - cost_b_minus_epsilon) / epsilon\n",
    "\n",
    "    return cost_a_finite_difference, cost_b_finite_difference\n",
    "\n",
    "\n",
    "def get_parameter_finite_difference_precision(x, y, a, b):\n",
    "    epsilon = 0.5\n",
    "\n",
    "    cost_a_minus_epsilon = get_cost_for_parameter(x, y, a - epsilon, b)\n",
    "    cost_b_minus_epsilon = get_cost_for_parameter(x, y, a, b - epsilon)\n",
    "    cost_a_plus_epsilon = get_cost_for_parameter(x, y, a + epsilon, b)\n",
    "    cost_b_plus_epsilon = get_cost_for_parameter(x, y, a, b + epsilon)\n",
    "\n",
    "    cost_a_finite_difference = (cost_a_plus_epsilon - cost_a_minus_epsilon) / (2 * epsilon)\n",
    "    cost_b_finite_difference = (cost_b_plus_epsilon - cost_b_minus_epsilon) / (2 * epsilon)\n",
    "\n",
    "    return cost_a_finite_difference, cost_b_finite_difference\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's race them !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optimizing parameter with \u001b[32mfinite_difference\u001b[39m\n",
      "Time taken: \u001b[33m2.256598949432373 seconds\u001b[39m \n",
      "\ta = 2.338760207696468\n",
      "\tb = 3.400629049669035\n",
      "\t\u001b[31mcost = 0.4308957239289341\u001b[39m\n",
      "\n",
      "\n",
      "Optimizing parameter with \u001b[32mderivative\u001b[39m\n",
      "Time taken: \u001b[33m1.4625835418701172 seconds\u001b[39m \n",
      "\ta = 2.6716733235463868\n",
      "\tb = 2.9896429548645806\n",
      "\t\u001b[31mcost = 0.33984662235534174\u001b[39m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------- Optimize --------------------------------- #\n",
    "\n",
    "\n",
    "def optimize_parameter(x, y, a, b, learning_rate, nb_iterations, derivative_method):\n",
    "    print(\n",
    "        f\"\\nOptimizing parameter with {colorama.Fore.GREEN}{derivative_method}{colorama.Fore.RESET}\"\n",
    "    )\n",
    "    start_time = time.time()\n",
    "    for _ in range(nb_iterations):\n",
    "        if derivative_method == \"derivative\":\n",
    "            d_cost_by_d_a, d_cost_by_d_b = get_parameter_derivative(x, y, a, b)\n",
    "        elif derivative_method == \"finite_difference\":\n",
    "            d_cost_by_d_a, d_cost_by_d_b = get_parameter_finite_difference(x, y, a, b)\n",
    "        elif derivative_method == \"finite_difference_precision\":\n",
    "            d_cost_by_d_a, d_cost_by_d_b = get_parameter_finite_difference_precision(\n",
    "                x, y, a, b\n",
    "            )\n",
    "        a = a - (learning_rate * d_cost_by_d_a)\n",
    "        b = b - (learning_rate * d_cost_by_d_b)\n",
    "    end_time = time.time()\n",
    "    cost = get_cost(y, predict(x, (a, b)))\n",
    "    print(\n",
    "        f\"Time taken: {colorama.Fore.YELLOW}{end_time - start_time} seconds{colorama.Fore.RESET} \\n\"\n",
    "        # f\"\\twith {derivative_method}\\n\"\n",
    "        f\"\\t{a = }\\n\"\n",
    "        f\"\\t{b = }\\n\"\n",
    "        f\"\\t{colorama.Fore.RED}{cost = }{colorama.Fore.RESET}\\n\"\n",
    "    )\n",
    "    return a, b\n",
    "\n",
    "\n",
    "# print(f\"IDEAL: {COEF_A = }\") # With the noise, it's not necessary true\n",
    "# print(f\"IDEAL: {COEF_B = }\") # With the noise, it's not necessary true\n",
    "learning_rate = 1e-4\n",
    "nb_iterations = 100_000\n",
    "\n",
    "\n",
    "a, b = optimize_parameter(\n",
    "    x,\n",
    "    y,\n",
    "    WORST_A_PARAM,\n",
    "    WORST_B_PARAM,\n",
    "    learning_rate,\n",
    "    nb_iterations,\n",
    "    \"finite_difference\",\n",
    ")\n",
    "# a, b = optimize_parameter(\n",
    "#     x, y, WORST_A_PARAM, WORST_B_PARAM, learning_rate, nb_iterations, \"finite_difference_precision\"\n",
    "# )\n",
    "a, b = optimize_parameter(\n",
    "    x, y, WORST_A_PARAM, WORST_B_PARAM, learning_rate, nb_iterations, \"derivative\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
